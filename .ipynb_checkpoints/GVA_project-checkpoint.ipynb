{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. INTRO\n",
    "\n",
    "#### What is GVA?\n",
    "\n",
    "Gross Value Added (GVA) is a measure of the increase in the value of the economy due to the production of goods and services. It is measured at current basic prices, which include the effect of inflation, excluding taxes (less subsidies) on products (for example, Value Added Tax). GVA plus taxes (less subsidies) on products is equivalent to Gross Domestic Product (GDP).\n",
    "\n",
    "Regional estimates of GVA are measured using the income approach, which involves adding up the income generated by resident individuals or corporations in the production of goods and services. ([source](http://webarchive.nationalarchives.gov.uk/20160106143511/http://www.ons.gov.uk/ons/dcp171778_388340.pdf))\n",
    "\n",
    "#### Dataset & goals of the project\n",
    "\n",
    "The dataset has been downloaded from the [Office for National Statistics website](https://www.ons.gov.uk/economy/grossvalueaddedgva/datasets/regionalgrossvalueaddedincomeapproach) and contains the following data:\n",
    "\n",
    "* Table 1: Gross Value Added (Income Approach) at current basic prices\n",
    "* Table 2: Gross Value Added (Income Approach) per head of population at current basic prices\n",
    "* Table 3: Gross Value Added (Income Approach) per head indices\n",
    "* Table 4: Growth in Gross Value Added (Income Approach)\n",
    "* Table 5: Growth in Gross Value Added (Income Approach) per head of population\n",
    "* Table 6: Gross Value Added (Income Approach) by SIC07 industry at current basic prices\n",
    "* Table 7: Compensation of Employees (CoE) by SIC07 industry at current basic prices\n",
    "* Table 8: Mixed Income by SIC07 industry at current basic prices\n",
    "* Table 9: Rental Income by SIC07 industry at current basic prices\n",
    "* Table 10: Non-Market Capital Consumption by SIC07 industry at current basic prices\n",
    "* Table 11: Holding Gains by SIC07 industry at current basic prices\n",
    "* Table 12: Gross Trading Profits by SIC07 industry at current basic prices\n",
    "* Table 13: Gross Trading Surplus by SIC07 industry at current basic prices\n",
    "* Table 14: Taxes on Production by SIC07 industry at current basic prices\n",
    "* Table 15: Subsidies on Production by SIC07 industry at current basic prices\n",
    "\n",
    "The goal of the excersise is to analyse how the current basic price for the GVA in Bath and the surrounding area has historically varied, and whether an accurate prediction can be made for future values using a statistical regression.\n",
    "\n",
    "I plan to use data from tab 1 as a target for my regression model and data from tabs 4 & 7-15 as explanatory features. Data in remaining tabs seem to be somewhat repetitive, only calculated in a different way - so using them would be reduntant and could lead to multicollinearity problems.\n",
    "\n",
    "### 2. EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "Initially I wanted to use indicators for every industry for the region however it quickly turned out it's a quite a bit of work to put the data together and would consume a lot of time out of my 6 hours assigned. I decided then to start with a simpler model that uses only local totals as the features, to be expanded if it doesn't bring satisfactory results and if time permits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import patsy\n",
    "import itertools\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"/Users/Mags/Desktop/bath_data.xlsx\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data dictionary:\n",
    "\n",
    "(all data refer to Bath region only)\n",
    "\n",
    "|feature|description|\n",
    "|---|---|\n",
    "|GVA_Bath|Gross Value Added (Income Approach) at current basic prices|\n",
    "|Growth_in_GVA_Bath|Growth in Gross Value Added (Income Approach)|\n",
    "|CoE_Bath|Compensation of Employees (CoE) at current basic prices|\n",
    "|Mix_Inc_Bath|Mixed Income at current basic prices|\n",
    "|Rent_Inc_Bath|Rental Income at current basic prices|\n",
    "|Cap_Con_Bath|Non-Market Capital Consumption at current basic prices|\n",
    "|Hold_Gains_Bath|Holding Gains at current basic prices|\n",
    "|GTP_Bath|Gross Trading Profits at current basic prices|\n",
    "|GTS_Bath|Gross Trading Profits at current basic prices|\n",
    "|Tax_Bath|Taxes on Production at current basic prices|\n",
    "|Subs_Bath|Subsidies on Production at current basic prices|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unecessary columns:\n",
    "data = data.drop([\"NUTS code\", \"Region name\"], axis=1)\n",
    "#resetting index:\n",
    "data = data.set_index(['Name'])\n",
    "data.index.name = None\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transposing dataframe for easier analysis\n",
    "data = data.T\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine the df:\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing data to numeric:\n",
    "data = data.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "\n",
    "There is just one missing value in Growth_in_GVA_Bath column - no data for year 1997. I presume this field is empty because we don't have data for 1996 and therefore no growth rate could be calculated, not because it is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's set the missing value to Nan and look at distribution of values in this column:\n",
    "\n",
    "data.replace('-',np.NaN, inplace=True)\n",
    "s = data.Growth_in_GVA_Bath\n",
    "s = s.dropna()\n",
    "plt.hist(s,\n",
    "         bins=6,\n",
    "         alpha=0.5,\n",
    "         color='#EDD834')\n",
    "plt.axvline(data.Growth_in_GVA_Bath.median(), color='r', linestyle='solid', linewidth=1, label = 'median')\n",
    "plt.axvline(data.Growth_in_GVA_Bath.mean(), color='b', linestyle='solid', linewidth=1, label = 'mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Median and mean seem quite close. Our sample size is small and we have just one missing value, \n",
    "#so we will replace it with the mean; \n",
    "#model-based imputaton seems a bit of overkill in this case.\n",
    "\n",
    "data.Growth_in_GVA_Bath = data.Growth_in_GVA_Bath.fillna(data.Growth_in_GVA_Bath.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical analysis and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining basic statistics:\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table we can see that the columns we have in the dataset differ very signifiantly when it comes to the magnitude of numbers. As I intend to run linear regression, for better interpretability of the results and also to make visualization easier, I'm going to standardize the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarization\n",
    "standard = StandardScaler()\n",
    "standard.fit(data)\n",
    "stand_data = pd.DataFrame(standard.transform(data),columns=data.columns)\n",
    "stand_data.index = data.index\n",
    "stand_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the target feature - GVA_Bath (non-standardized values)\n",
    "plt.figure(figsize = (8, 5))\n",
    "plt.plot(data.index, data.GVA_Bath, lw =4.5, c='#AD2C5F')\n",
    "plt.xlim(1997,2015)\n",
    "plt.ylim(6000, 20000)\n",
    "plt.xticks(data.index.tolist(), rotation=45)\n",
    "plt.legend(cols, loc='upper left')\n",
    "plt.ylabel(\"GVA value\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows how GVA changed historically in the Bath area. We can see a steady growth over the years with just one significant dip from 2008 to 2009, probably due to the financial crisis of the late nougthies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all features (standardized)\n",
    "cols = stand_data.columns.tolist()\n",
    "colours = ['red','skyblue', 'sage', 'purple', 'mistyrose', 'darkolivegreen',\n",
    "            'tomato', 'greenyellow', 'burlywood', 'mediumspringgreen']\n",
    "\n",
    "plt.figure(figsize = (11, 7))\n",
    "#plt.plot(stand_data.index, stand_data.GVA_Bath, lw =4.5, c='#AD2C5F')\n",
    "for column, colour in zip(cols, colours):\n",
    "    plt.plot(stand_data.index, stand_data[column], c= colour)\n",
    "\n",
    "plt.xlim(1997,2015)\n",
    "plt.xticks(stand_data.index.tolist(), rotation=45)\n",
    "plt.legend(cols, loc='upper left')\n",
    "plt.ylabel(\"Standardized feature values\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above chart we can see that while some features (Tax, CoE, Rent_Inc etc.) seem to follow similar steady growth trend as GVA (and as such they could be considered potentially good predictors for GVA levels), while other features (Growth, Hold_Gains, GTS) seem to be far more varied with lot of ups and downs over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating p_values:\n",
    "p_values = f_regression(X, y, center=True)[1]\n",
    "p_vals=[\"{0:.7f}\".format(x)for x in p_values] \n",
    "\n",
    "for f, p in zip (featurenames, p_vals):\n",
    "    print f, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-values obtained for our features suggest that the changes in variables with more erratic diagrams, as above mentioned Growth, Hold_Gains, and in lesser degree GTS, are not associated with changes in the target. Nevertheless I'm going to keep them as I don't believe significance alone is enough reason to remove them at this stage and the p-values obtained could be misleading due to the small sample size.\n",
    "\n",
    "To see how well the features correlate let's build a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7, 5))\n",
    "sns.heatmap(stand_data.ix[:,'GVA_Bath':].corr(), annot = stand_data.ix[:,'GVA_Bath':].corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some very high correlation coefficients. I presumed the variables I'm using are exogenous in a sense that they are not derived from each other but perhaps it's not the case. CoE and Tax seem to be particularly well correlated with other features and the target but I would need more time to explore the underlying correlational structure of the data properly. For now I'm going to leave them as they are, bearing in mind that I might need to remedy this problem at a later stage.\n",
    "\n",
    "Next, let's look at the pairplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(stand_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, some features have very strong linear relationship, especially CoE & Tax so we will need to consider removing them from the set or reengineering them if we would like to use the model not only for predicitons, but also for explaning which features influence the target the most. \n",
    "\n",
    "### MODELLING\n",
    "\n",
    "Normally the best practice would be to divide the dataset into train and test sets and validate model's quality by training it on the train set and testing it on a test set. Our dataset however has less than 20 observations therefore splitting it into train and test sets would result in a very small number of obserations in both sets. I'm going to use k-fold cross-validation instead and while I know that the best practice is to set aside a test set even when using cross-validation, I'm going to skip this step due to minimal amount of data.\n",
    "\n",
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the target and feature matrix\n",
    "\n",
    "st_data_cols = stand_data.columns\n",
    "\n",
    "y= stand_data.GVA_Bath\n",
    "\n",
    "X = stand_data[st_data_cols[1:]]\n",
    "\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(X, y)\n",
    "\n",
    "# Perform 10-fold cross validation\n",
    "scores = cross_val_score(lm, X, y, cv=10)\n",
    "print \"Cross-validated R2 scores:\", scores\n",
    "print \"Mean of cross-validated R2 scores:\", scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the coefficients:\n",
    "\n",
    "featurenames = X.columns\n",
    "\n",
    "for f, c in zip(featurenames, lm.coef_):\n",
    "    print f, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(featurenames, lm.coef_, ci=95, orient=None, \n",
    "            color='#a12957', palette=None, saturation=0.75)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Coefficient value\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylim(0,.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary with the gridsearch parameters\n",
    "params = {'alpha': np.logspace(-10,10,100)}\n",
    "\n",
    "#Create and Fit a Lasso Regression Model, Testing Each Alpha\n",
    "\n",
    "grid_lasso = GridSearchCV(Lasso(), params, cv=10)\n",
    "grid_lasso.fit(X,y)\n",
    "\n",
    "#Summarize the Results of the Grid Search\n",
    "print \"Best score:\\n\", grid_lasso.best_score_\n",
    "print \"Best estimator alpha: \\n\", grid_lasso.best_estimator_.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, c in zip(featurenames, grid_lasso.best_estimator_.coef_):\n",
    "    print f, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(featurenames, grid_lasso.best_estimator_.coef_, ci=95, orient=None, \n",
    "            color='#0073e6', palette=None, saturation=0.75)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Coefficient value\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylim(0,.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso ended up supressing Subs feature, the rest of coefficients look similar. Its best score is higher than the regular linear regression's score.\n",
    "\n",
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary with the gridsearch parameters\n",
    "params = {'alpha': np.logspace(-10,10,100)}\n",
    "\n",
    "#Create and Fit a Ridge Regression Model, Testing Each Alpha\n",
    "\n",
    "grid_ridge = GridSearchCV(Ridge(), params, cv=10)\n",
    "grid_ridge.fit(X,y)\n",
    "\n",
    "#Summarize the Results of the Grid Search\n",
    "print \"Best score:\\n\", grid_ridge.best_score_\n",
    "print \"Best estimator alpha: \\n\", grid_ridge.best_estimator_.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, c in zip(featurenames, grid_ridge.best_estimator_.coef_):\n",
    "    print f, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(featurenames, grid_ridge.best_estimator_.coef_, ci=95, orient=None, \n",
    "            color='#cc7a00', palette=None, saturation=0.75)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Coefficient value\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylim(0,.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge's results are very similar to regular linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY\n",
    "\n",
    "Having performed quick analysis we can confirm that there is a clear growth trend in GVA in Bath area in years 1997 - 2016. \n",
    "\n",
    "All three models worked well as far as we can say without testing them, with similar R2 scores from .83 in case of logistic regression to .88 in case of Ridge. For all models, considering all other variables fixed, CeO os the feature which has the biggest impact on GVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
